#!/usr/bin/python3

import os
import sys
import yaml
import logging
import argparse

lib_path = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
sys.path.append(lib_path)

from ccquery.error import ConfigError, CaughtException
from ccquery.utils import io_utils, cfg_utils
from ccquery.preprocessing import WikiExtraction


#=============================================
# Parse the command line arguments
#=============================================

options = {}
parser = argparse.ArgumentParser(
    description='Download, decompress, extract and clean wikipedia dump data')
parser.add_argument('conf', help='input config file (yml)')
options = parser.parse_args()

#=============================================
# Logger setup
#=============================================

logger = logging.getLogger('ccquery')

#=============================================
# Load and check configuration
#=============================================

conf = cfg_utils.load_configuration(options.conf)
logger.info("Processing configuration: {}".format(conf))

cfg_utils.match_keys(conf, ['url', 'output'])

# conf['url']     url for wikipedia .bz2 dump
# conf['args']    arguments for the WikiExtractor script
# conf['output']  absolute path for the output folder

io_utils.create_folder(conf['output'])

# Wikipedia extraction options
# Possible usefull commands:
# --quiet => do not show progress info
# --json => write output in json format
# --bytes 30G => maximum bytes per output file (default 1M)
# --processes 2 => number of processes to use (default all CPUs)
# --no-templates => do not expand templates
# --filter_disambig_pages => ignore pages that contain disabmiguation markup
# --min_text_length 50 => keep documents with minimum expanded text length
args = conf.get('args', [])

# options for sentence preprocessing
preprocessing = conf.get(
    'preprocessing',
    {
        'ignore_digits': True,
        'apostrophe': 'fr',
        'ignore_punctuation': 'noise-a',
        'tostrip': True,
        'keepalnum': True
    })

# options for word and char occurrences plot
plot_format = conf.get('plot_format', 'png')
word_plot_kwargs = conf.get('word_plot')
char_plot_kwargs = conf.get('char_plot')

# options for word occurrences filter
word_filter = conf.get('word_filter', {'topn': 100000})
filteropt = '='.join(str(x) for x in list(word_filter.items())[0])

#=============================================
# Process Wikipedia dump
#=============================================

def add_extension(path, ext):
    return "{}{}".format(path, ext)

fn = os.path.join(conf['output'], io_utils.basename(conf['url']).split('.')[0])

files = {
    'url':   conf['url'],
    'bz2':   add_extension(fn, '.bz2'),
    'xml':   add_extension(fn, '.xml'),
    'jsonl': add_extension(fn, '.jsonl'),
    'txt':   add_extension(fn, '.txt'),
    'wvoc':  add_extension(fn, "_voc-{}-words.json".format(filteropt)),
    'cvoc':  add_extension(fn, '_voc-chars.json'),
}

wiki = WikiExtraction()

logger.info('Download wikipedia dump')
wiki.save_archive(files['url'], files['bz2'])

logger.info('Decompress data')
wiki.save_xml(files['bz2'], files['xml'])

logger.info('Extract plain text')
wiki.save_content(files['xml'], files['jsonl'], args)

logger.info('Extract clean sentences')
wiki.save_sentences(files['jsonl'], files['txt'], 'text', **preprocessing)

# process words

logger.info('Process words')
wiki.load_words(files['txt'])

if word_plot_kwargs:
    logger.info('Plot word occurrences')
    plot_file = add_extension(fn, "_words.{}".format(plot_format))
    wiki.plot_word_occurrences(plot_file, **word_plot_kwargs)

logger.info('Filter words with respect to number of occurrences')
wiki.filter_words(**word_filter)

logger.info('Save word vocabulary under json and txt format')
wiki.save_words(files['wvoc'])
wiki.save_words(io_utils.change_extension(files['wvoc'], 'txt'))

# process characters

logger.info('Process characters')
wiki.load_chars(files['txt'])

if char_plot_kwargs:
    logger.info('Plot character occurrences')
    plot_file = add_extension(fn, "_chars.{}".format(plot_format))
    wiki.plot_char_occurrences(plot_file, **char_plot_kwargs)

logger.info('Save character vocabulary under json and txt format')
wiki.save_chars(files['cvoc'])
wiki.save_chars(io_utils.change_extension(files['cvoc'], 'txt'))

logger.info('Finished.')
